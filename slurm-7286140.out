/var/spool/slurm/job7286140/slurm_script:9: command not found: module

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


{0: [0], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:51:43,396 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:51:43,399 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 29.7MB/s]
[INFO|configuration_utils.py:737] 2024-11-08 10:51:43,841 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:51:43,846 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:51:43,878 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:51:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:51:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_0_initfixed/runs/Nov08_10-51-40_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_0_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_0_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [1], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:51:51,182 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:51:51,184 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:51:51,493 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:51:51,497 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:51:51,512 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:51:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:51:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_1_initfixed/runs/Nov08_10-51-48_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_1_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_1_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [2], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:51:57,860 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:51:57,862 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:51:58,172 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:51:58,176 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:51:58,190 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:51:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:51:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_2_initfixed/runs/Nov08_10-51-55_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_2_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_2_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [3], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:04,429 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:04,431 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:04,735 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:04,739 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:04,753 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:02 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_3_initfixed/runs/Nov08_10-52-01_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_3_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_3_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [4], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:10,954 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:10,957 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:11,261 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:11,265 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:11,280 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:08 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_4_initfixed/runs/Nov08_10-52-08_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_4_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_4_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [5], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:17,696 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:17,698 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:18,007 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:18,011 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:18,025 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_5_initfixed/runs/Nov08_10-52-15_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_5_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_0_5_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [0], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:23,922 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:23,924 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:24,229 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:24,232 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:24,244 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:22 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_0_initfixed/runs/Nov08_10-52-21_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_0_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_0_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [1], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:30,260 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:30,262 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:30,598 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:30,602 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:30,617 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_1_initfixed/runs/Nov08_10-52-27_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_1_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_1_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [2], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:36,970 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:36,971 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:37,287 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:37,291 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:37,305 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:34 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_2_initfixed/runs/Nov08_10-52-34_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_2_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_2_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [3], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:43,953 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:43,956 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:44,268 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:44,272 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:44,285 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:41 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:41 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_3_initfixed/runs/Nov08_10-52-40_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_3_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_3_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [4], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:50,320 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:50,322 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:50,619 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:50,622 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:50,634 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_4_initfixed/runs/Nov08_10-52-47_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_4_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_4_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [5], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:52:56,880 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:52:56,882 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:52:57,182 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:52:57,185 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:52:57,198 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:52:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:52:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_5_initfixed/runs/Nov08_10-52-54_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_5_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_1_5_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [0], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:03,117 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:03,118 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:03,418 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:03,421 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:03,433 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_0_initfixed/runs/Nov08_10-53-00_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_0_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_0_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [1], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:09,614 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:09,616 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:09,930 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:09,934 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:09,951 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_1_initfixed/runs/Nov08_10-53-06_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_1_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_1_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [2], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:16,025 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:16,027 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:16,340 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:16,343 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:16,356 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_2_initfixed/runs/Nov08_10-53-13_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_2_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_2_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [3], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:22,638 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:22,640 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:22,956 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:22,960 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:22,974 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_3_initfixed/runs/Nov08_10-53-20_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_3_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_3_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [4], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:29,077 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:29,079 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:29,430 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:29,434 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:29,446 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_4_initfixed/runs/Nov08_10-53-26_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_4_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_4_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [5], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:35,800 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:35,802 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:36,123 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:36,127 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:36,141 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:33 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_5_initfixed/runs/Nov08_10-53-33_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_5_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_2_5_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [], 3: [0], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:42,467 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:42,469 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:42,855 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:42,859 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:42,872 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_0_initfixed/runs/Nov08_10-53-39_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_0_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_0_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [], 3: [1], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:49,366 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:49,368 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:49,684 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:49,688 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:49,702 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_1_initfixed/runs/Nov08_10-53-46_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_1_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_1_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [], 3: [2], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:53:55,692 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:53:55,694 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:53:56,014 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:53:56,017 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:53:56,029 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:53:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:53:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_2_initfixed/runs/Nov08_10-53-53_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_2_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_2_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [], 3: [3], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:54:02,198 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:54:02,200 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:54:02,578 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:54:02,582 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:54:02,596 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:54:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:54:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_3_initfixed/runs/Nov08_10-53-59_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_3_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_3_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [], 3: [4], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:54:08,688 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:54:08,690 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:54:08,997 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:54:09,001 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:54:09,013 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:54:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:54:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_4_initfixed/runs/Nov08_10-54-06_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_4_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_4_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [], 3: [5], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
Result 44
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py:218: FutureWarning: The argument `--freeze_feature_extractor` is deprecated and will be removed in a future version. Use `--freeze_feature_encoder` instead. Setting `freeze_feature_encoder==True`.
  warnings.warn(
/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for speechbrain/common_language contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechbrain/common_language
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
[INFO|feature_extraction_utils.py:535] 2024-11-08 10:54:15,049 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/preprocessor_config.json
[INFO|feature_extraction_utils.py:579] 2024-11-08 10:54:15,051 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 30,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 80,
  "hop_length": 160,
  "n_fft": 400,
  "n_samples": 480000,
  "nb_max_frames": 3000,
  "padding_side": "right",
  "padding_value": 0.0,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "sampling_rate": 16000
}

[INFO|configuration_utils.py:737] 2024-11-08 10:54:15,430 >> loading configuration file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/config.json
[INFO|configuration_utils.py:802] 2024-11-08 10:54:15,434 >> Model config WhisperConfig {
  "_name_or_path": "/scratch/gilbreth/amohanpa/Whisper-tiny/common_language",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "apply_spec_augment": false,
  "architectures": [
    "WhisperForAudioClassification"
  ],
  "attention_dropout": 0.0,
  "begin_suppress_tokens": [
    220,
    50257
  ],
  "bos_token_id": 50257,
  "classifier_proj_size": 256,
  "d_model": 384,
  "decoder_attention_heads": 6,
  "decoder_ffn_dim": 1536,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 4,
  "decoder_start_token_id": 50258,
  "dropout": 0.0,
  "encoder_attention_heads": 6,
  "encoder_ffn_dim": 1536,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 4,
  "eos_token_id": 50257,
  "finetuning_task": "audio-classification",
  "forced_decoder_ids": [
    [
      1,
      50259
    ],
    [
      2,
      50359
    ],
    [
      3,
      50363
    ]
  ],
  "id2label": {
    "0": "Arabic",
    "1": "Basque",
    "10": "Dutch",
    "11": "English",
    "12": "Esperanto",
    "13": "Estonian",
    "14": "French",
    "15": "Frisian",
    "16": "Georgian",
    "17": "German",
    "18": "Greek",
    "19": "Hakha_Chin",
    "2": "Breton",
    "20": "Indonesian",
    "21": "Interlingua",
    "22": "Italian",
    "23": "Japanese",
    "24": "Kabyle",
    "25": "Kinyarwanda",
    "26": "Kyrgyz",
    "27": "Latvian",
    "28": "Maltese",
    "29": "Mangolian",
    "3": "Catalan",
    "30": "Persian",
    "31": "Polish",
    "32": "Portuguese",
    "33": "Romanian",
    "34": "Romansh_Sursilvan",
    "35": "Russian",
    "36": "Sakha",
    "37": "Slovenian",
    "38": "Spanish",
    "39": "Swedish",
    "4": "Chinese_China",
    "40": "Tamil",
    "41": "Tatar",
    "42": "Turkish",
    "43": "Ukranian",
    "44": "Welsh",
    "5": "Chinese_Hongkong",
    "6": "Chinese_Taiwan",
    "7": "Chuvash",
    "8": "Czech",
    "9": "Dhivehi"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "Arabic": "0",
    "Basque": "1",
    "Breton": "2",
    "Catalan": "3",
    "Chinese_China": "4",
    "Chinese_Hongkong": "5",
    "Chinese_Taiwan": "6",
    "Chuvash": "7",
    "Czech": "8",
    "Dhivehi": "9",
    "Dutch": "10",
    "English": "11",
    "Esperanto": "12",
    "Estonian": "13",
    "French": "14",
    "Frisian": "15",
    "Georgian": "16",
    "German": "17",
    "Greek": "18",
    "Hakha_Chin": "19",
    "Indonesian": "20",
    "Interlingua": "21",
    "Italian": "22",
    "Japanese": "23",
    "Kabyle": "24",
    "Kinyarwanda": "25",
    "Kyrgyz": "26",
    "Latvian": "27",
    "Maltese": "28",
    "Mangolian": "29",
    "Persian": "30",
    "Polish": "31",
    "Portuguese": "32",
    "Romanian": "33",
    "Romansh_Sursilvan": "34",
    "Russian": "35",
    "Sakha": "36",
    "Slovenian": "37",
    "Spanish": "38",
    "Swedish": "39",
    "Tamil": "40",
    "Tatar": "41",
    "Turkish": "42",
    "Ukranian": "43",
    "Welsh": "44"
  },
  "mask_feature_length": 10,
  "mask_feature_min_masks": 0,
  "mask_feature_prob": 0.0,
  "mask_time_length": 10,
  "mask_time_min_masks": 2,
  "mask_time_prob": 0.05,
  "max_length": 448,
  "max_source_positions": 1500,
  "max_target_positions": 448,
  "median_filter_width": 7,
  "model_type": "whisper",
  "num_hidden_layers": 4,
  "num_mel_bins": 80,
  "pad_token_id": 50257,
  "scale_embedding": false,
  "suppress_tokens": [
    1,
    2,
    7,
    8,
    9,
    10,
    14,
    25,
    26,
    27,
    28,
    29,
    31,
    58,
    59,
    60,
    61,
    62,
    63,
    90,
    91,
    92,
    93,
    359,
    503,
    522,
    542,
    873,
    893,
    902,
    918,
    922,
    931,
    1350,
    1853,
    1982,
    2460,
    2627,
    3246,
    3253,
    3268,
    3536,
    3846,
    3961,
    4183,
    4667,
    6585,
    6647,
    7273,
    9061,
    9383,
    10428,
    10929,
    11938,
    12033,
    12331,
    12562,
    13793,
    14157,
    14635,
    15265,
    15618,
    16553,
    16604,
    18362,
    18956,
    20075,
    21675,
    22520,
    26130,
    26161,
    26435,
    28279,
    29464,
    31650,
    32302,
    32470,
    36865,
    42863,
    47425,
    49870,
    50254,
    50258,
    50358,
    50359,
    50360,
    50361,
    50362
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.36.0",
  "use_cache": true,
  "use_weighted_layer_sum": false,
  "vocab_size": 51865
}

[INFO|modeling_utils.py:3329] 2024-11-08 10:54:15,451 >> loading weights file /scratch/gilbreth/amohanpa/Whisper-tiny/common_language/model.safetensors
Traceback (most recent call last):
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 564, in <module>
    main()
  File "/home/amohanpa/transformers/examples/pytorch/audio-classification/run_audio_classification.py", line 418, in main
    model = AutoModelForAudioClassification.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3694, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amohanpa/.conda/envs/cent7/2020.11-py38/Generation/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4154, in _load_pretrained_model
    raise RuntimeError(f"Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}")
RuntimeError: Error(s) in loading state_dict for WhisperForAudioClassification:
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.0.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.1.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.2.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.1.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.3.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.4.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.weight: copying a param with shape torch.Size([16, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 1, 1, 7]).
	size mismatch for encoder.layers.3.self_attn.softmax_16.5.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).
	You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
<class 'str'>
44
['11/08/2024 10:54:13 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False', '11/08/2024 10:54:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(', '_n_gpu=1,', 'adafactor=False,', 'adam_beta1=0.9,', 'adam_beta2=0.999,', 'adam_epsilon=1e-08,', 'auto_find_batch_size=False,', 'bf16=False,', 'bf16_full_eval=False,', 'data_seed=None,', 'dataloader_drop_last=False,', 'dataloader_num_workers=1,', 'dataloader_persistent_workers=False,', 'dataloader_pin_memory=True,', 'ddp_backend=None,', 'ddp_broadcast_buffers=None,', 'ddp_bucket_cap_mb=None,', 'ddp_find_unused_parameters=None,', 'ddp_timeout=1800,', 'debug=[],', 'deepspeed=None,', 'disable_tqdm=False,', 'dispatch_batches=None,', 'do_eval=True,', 'do_predict=False,', 'do_train=True,', 'eval_accumulation_steps=None,', 'eval_delay=0,', 'eval_steps=None,', 'evaluation_strategy=no,', 'fp16=False,', 'fp16_backend=auto,', 'fp16_full_eval=False,', 'fp16_opt_level=O1,', 'fsdp=[],', "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},", 'fsdp_min_num_params=0,', 'fsdp_transformer_layer_cls_to_wrap=None,', 'full_determinism=False,', 'gradient_accumulation_steps=2,', 'gradient_checkpointing=False,', 'gradient_checkpointing_kwargs=None,', 'greater_is_better=None,', 'group_by_length=False,', 'half_precision_backend=auto,', 'hub_always_push=False,', 'hub_model_id=None,', 'hub_private_repo=False,', 'hub_strategy=every_save,', 'hub_token=<HUB_TOKEN>,', 'ignore_data_skip=False,', 'include_inputs_for_metrics=False,', 'include_num_input_tokens_seen=False,', 'include_tokens_per_second=False,', 'jit_mode_eval=False,', 'label_names=None,', 'label_smoothing_factor=0.0,', 'learning_rate=0.187,', 'length_column_name=length,', 'load_best_model_at_end=False,', 'local_rank=0,', 'log_level=passive,', 'log_level_replica=warning,', 'log_on_each_node=True,', 'logging_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_5_initfixed/runs/Nov08_10-54-12_gilbreth-m000.rcac.purdue.edu,', 'logging_first_step=False,', 'logging_nan_inf_filter=True,', 'logging_steps=25,', 'logging_strategy=steps,', 'lr_scheduler_kwargs={},', 'lr_scheduler_type=linear,', 'max_grad_norm=1.0,', 'max_steps=-1,', 'metric_for_best_model=None,', 'mp_parameters=,', 'neftune_noise_alpha=None,', 'no_cuda=False,', 'num_train_epochs=1.0,', 'optim=adamw_torch,', 'optim_args=None,', 'output_dir=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_5_initfixed/,', 'overwrite_output_dir=True,', 'past_index=-1,', 'per_device_eval_batch_size=64,', 'per_device_train_batch_size=32,', 'prediction_loss_only=False,', 'push_to_hub=False,', 'push_to_hub_model_id=None,', 'push_to_hub_organization=None,', 'push_to_hub_token=<PUSH_TO_HUB_TOKEN>,', 'ray_scope=last,', 'remove_unused_columns=False,', "report_to=['wandb'],", 'resume_from_checkpoint=None,', 'run_name=/scratch/gilbreth/amohanpa/Whisper-tiny/common_language/Simple/WithInit/Thresh168/head-by-head-seq_3_5_initfixed/,', 'save_on_each_node=False,', 'save_only_model=False,', 'save_safetensors=True,', 'save_steps=500,', 'save_strategy=steps,', 'save_total_limit=None,', 'seed=42,', 'skip_memory_metrics=True,', 'split_batches=False,', 'tf32=None,', 'torch_compile=False,', 'torch_compile_backend=None,', 'torch_compile_mode=None,', 'torchdynamo=None,', 'tpu_metrics_debug=False,', 'tpu_num_cores=None,', 'use_cpu=False,', 'use_ipex=False,', 'use_legacy_prediction_loop=False,', 'use_mps_device=False,', 'warmup_ratio=0.0,', 'warmup_steps=0,', 'weight_decay=0.0,', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44']
<class 'float'>
{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}
/scratch/gilbreth/amohanpa/Whisper-tiny/common_language
